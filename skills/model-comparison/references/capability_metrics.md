# 能力评估指标说明

## 评分体系

本技能采用 **1-10 分制**进行能力评估，评分标准如下：

| 分值 | 等级 | 说明 |
|------|------|------|
| 9-10 | 卓越 | 业界领先水平，几乎无明显短板 |
| 7-8 | 优秀 | 高于平均水平，有明显优势 |
| 5-6 | 良好 | 满足一般需求，表现稳定 |
| 3-4 | 一般 | 存在明显不足，特定场景可用 |
| 1-2 | 较弱 | 需要改进，不建议作为首选 |

---

## 能力维度定义

### 1. 代码能力

评估模型在以下任务中的表现：

- **代码生成**：根据需求生成完整、可运行的代码
- **代码理解**：解读现有代码逻辑、意图
- **调试修复**：识别并修复代码中的错误
- **代码优化**：改进代码质量和性能
- **多语言编程**：支持多种编程语言

**基准测试**：
- HumanEval、MBPP、Codeforces

### 2. 数学推理

评估模型在以下任务中的表现：

- **算术计算**：基本数学运算的准确性
- **公式推导**：数学公式的理解和推导
- **逻辑证明**：数学证明题的解答
- **应用题**：将实际问题转化为数学模型

**基准测试**：
- GSM8K、MATH、CMMLU（数学子集）

### 3. 中文理解

评估模型对中文的处理能力：

- **阅读理解**：中文文章的理解和问答
- **写作生成**：中文文章的创作质量
- **成语/典故**：中文文化背景知识
- **语义理解**：中文歧义句、多义词的解析

**基准测试**：
- CMMLU、C-Eval、CLUE

### 4. 多语言能力

评估模型处理多种语言的能力：

- **英文理解**：英文阅读、写作
- **翻译能力**：多语言互译
- **跨语言理解**：多语言混合内容的处理
- **小语种支持**：非主流语言的处理

**基准测试**：
- MMLU（英文）、 Flores-100（翻译）

### 5. 长上下文

评估模型处理长文本的能力：

- **上下文窗口**：支持的 token 数量
- **信息召回**：长文中细节信息的提取
- **全局理解**：跨长文的逻辑连贯性
- **注意力机制**：长程依赖的处理

**评估方法**：
- 大海捞针测试（Needle Test）
- 长文档问答准确率

### 6. 逻辑推理

评估模型的推理能力：

- **演绎推理**：从一般到特殊的推理
- **归纳推理**：从特殊到一般的推理
- **类比推理**：相似性推理
- **因果推理**：因果关系的识别和分析

**基准测试**：
- LogiQA、ReClor、BBH

### 7. 工具调用

评估模型调用外部工具的能力：

- **API 调用**：正确调用外部 API
- **函数执行**：执行预定义函数
- **工具选择**：选择合适的工具完成任务
- **参数传递**：正确传递调用参数

**评估方法**：
- API-Bench、ToolLLM

### 8. 成本效率

综合评估使用成本：

- **API 价格**：单位 token 的价格
- **响应速度**：生成速度（tokens/秒）
- **性价比**：能力/价格比
- **资源消耗**：推理时的资源占用

---

## 权重配置

根据不同场景，可以调整各维度的权重：

```python
# 代码开发场景权重
CODE_WEIGHTS = {
    "代码能力": 0.35,
    "数学推理": 0.15,
    "逻辑推理": 0.20,
    "工具调用": 0.15,
    "其他": 0.15,
}

# 长文档处理权重
DOC_WEIGHTS = {
    "长上下文": 0.30,
    "中文理解": 0.20,
    "逻辑推理": 0.20,
    "多语言能力": 0.15,
    "其他": 0.15,
}
```
